# 深度学习与深层神经网络
深度学习定义为一类通过多层非线性变换对高复杂性数据建模算法的合集。由定义知，其两个重要的特征就是多层和非线性。

线性模型的最大特点是任意线性模型的组合仍然还是线性模型，只通过线性变换，任意层的全连接神经网络和单层神经网络模型的表达能力没有任何区别。

如果将每一个神经元的输出通过一个非线性函数，那么整个神经网络的模型也就不再是线性的了，这个非线性函数就是激活函数。
# 损失函数定义
交叉熵（cross entropy）刻画了两个概率分布之间的距离，是分类问题中使用比较广德一种损失函数。给定两个概率分布p和q，通过q来表示p的交叉熵为：
$$H(p,q) = - \sum p(x)\log q(x)$$
交叉熵函数不是对称的，它刻画的是通过概率分布q来表达概率分布p的困难程度。当交叉熵作为神经网络的损失函数时，p代表的是正确答案，q代表的是预测值。

Softmax回归是一个常用的将神经网络前向传播得到的结果变成概率分布的方法。可以通过下面的方式实现交叉熵
```python
import tensorflow as tf
cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))
```
其中`y_`代表正确结果，`y`代表预测结果。通过`tf.clip_by_value`函数可以将张量中的数值限制在一个范围内。`tf.log`函数完成对张量中所有元素依次求对数。`tf.reduce_mean`直接对整个矩阵做平均。由于交叉熵一般与`softmax`回归一起使用，`tensorflow`对这两个功能就行了统一封装。
```python
cross_entropy = tf.nn.softmax_cross_entropy_with_logits(y, y_)
```
对于回归问题，最常用的损失函数是均方误差（MSE, mean squared error）。定义如下：
$$MSE(y,\hat y) = \frac {\sum_{i=1}^n (y_i - \hat y_i)^2 }{n}$$
其中$y_i$为一个batch中第i个数据的正确答案，而$1*\hat y_i$为神经网络给出的预测值。
使用tensorflow实现如下：
```python
mse = tf.reduce_mean(tf.square(y_ - y))
```
